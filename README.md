# SRMIST_23VI35SRM_Reward_Model_for_Products
SRIB-PRISM_Program

# Reward Model for Products
## Project Overview
This project addresses the challenge of bias in Language Models (LLMs) by developing a reward model to classify responses as safe or unsafe with respect to Product A. The motivation stems from the tendency of LLMs to produce biased or prejudiced content, even with safeguards in place. The goal is to enhance the quality of responses and mitigate biases through a comprehensive approach involving reinforcement learning and human feedback.

## Progress Updates
### 1st Month Progress
#### Literature Review:

Read state-of-the-art (SOTA) papers to understand the current landscape of bias mitigation in language models.
Machine Learning Techniques:

Acquired knowledge of various machine learning techniques relevant to the project.
#### Data Preparation:

Modified the reference dataset using augmentation techniques to create a new dataset suitable for the project.
### 2nd Month Progress
#### Model Exploration:

Explored various models for the project.
#### Testing:

Ran tests on the dataset using the following models:
Linear Regression
Gaussian Naive Bayes
Multinomial Naive Bayes
Bernoulli Naive Bayes
Word2Vec
#### Model Accuracy:

Gaussian NB: 66%, 
Multinomial NB: 68%, 
Bernoulli NB: 69%, 
Word2Vec: 79%

### 3rd Month Progress
#### Model Exploration:

Explored various models for the project.
#### Testing:

Ran tests on the dataset using the following models:
RandomForestClassifier
#### Model Accuracy:

RandomForestClassifier: 99%

### 4th Month Progress
#### Model Exploration:

Explored various models for the project.
#### Testing:

Ran tests on the dataset using the following models:
Bert Models(Small and Large)
#### Model Accuracy:

Bert Large-67%

### 5th Month Progress
#### Work Progress:

Fine tuned the model with another similar dataset that is available on hugging face.
#### Testing:

#### Model Accuracy:

Fine Tuned Model: 82%
